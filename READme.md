
# üë®‚Äçüíª Moacir Fernandes

### FullStack Developer | Front-End UI/UX Specialist | AI Agent Integrator

Hello! I'm **Moacir Fernandes**, a FullStack developer who merges intuitive design with solid architecture. With strong experience in **front-end UI/UX development**, **cloud deployment**, and now **local AI agent integration**, I deliver elegant and efficient solutions that scale from concept to production.

---

## üöÄ Latest Feature: Local AI Agent Powered by Ollama

This portfolio now features a real-time, **local AI-powered agent** using [Ollama](https://ollama.com), an open-source LLM runtime that allows you to run large language models on your own machine without relying on cloud APIs.

### üîå What It Does
- üåê Runs LLMs locally (no API costs or network dependency)
- ü§ñ Integrates models like `gemma:2b` for natural language tasks
- üß† Enables multi-role agents: **Ideator**, **Coder**, and **Reviewer**
- üí¨ Interacts via a modal interface inside the portfolio

### üõ†Ô∏è How It Works
- Backend (`Express.js`) runs on `http://localhost:3001`
- Routes `/api/agent` proxy user messages to Ollama (`http://localhost:11434/api/generate`)
- Model: `gemma:2b` pulled locally and served
- Front-end React component handles UX and displays responses

---

## üß† Tech Stack Overview

### üé® Front-End (UI/UX-Focused)
- **React.js**, **Vite**, **TypeScript**
- **Styled Components**, **CSS Modules**
- **Framer Motion**, **Custom Modals**
- Responsive, animated, accessible

### ‚öôÔ∏è Back-End & APIs
- **Node.js**, **Express**
- **Custom API Routing**
- **Ollama integration via HTTP POST**

### ü§ñ AI & LLM Tools
- **Ollama**: lightweight local inference runtime
- **gemma:2b** model (Google-backed LLM)
- Agent roles: **Ideator**, **Coder**, **Reviewer**
- Prompt-to-response integration without cloud billing

---

## üíª Folder Structure Highlight

```
/api
  ‚îî‚îÄ‚îÄ server.js         ‚Üê Express server w/ Ollama integration
/src/components
  ‚îî‚îÄ‚îÄ AgentModal.jsx    ‚Üê Talk to Agent UI logic
```

---

## üí° Real Use Case

This portfolio showcases how you can **embed autonomous agents directly into user interfaces**, making it perfect for:

- Portfolios and product showcases
- Internal dashboards
- AI assistants without API dependency

---

## üì´ Let's Connect

- üìß Email: [moacirsistemax@gmail.com](mailto:moacirsistemax@gmail.com)
- üßë‚Äçüíª GitHub: [@Moa-fernandes](https://github.com/Moa-fernandes)
- üíº LinkedIn: [Moacir Fernandes](https://www.linkedin.com/in/moacir-fernandes-ba0a97a0/)


---


# üë®‚Äçüíª Moacir Fernandes

### Desenvolvedor FullStack | Especialista em UI/UX | Integrador de Agentes de IA Locais

Sou **Moacir Fernandes**, desenvolvedor fullstack com paix√£o por design e engenharia, agora incorporando agentes de IA locais com **Ollama** diretamente no meu portf√≥lio ‚Äî sem depend√™ncia de APIs externas.

---

## ü§ñ Recurso Mais Recente: Agente de IA Local com Ollama

Este portf√≥lio conta agora com um **agente de IA local** que roda diretamente na sua m√°quina usando [Ollama](https://ollama.com), ideal para testes r√°pidos, sem limites ou custos de API.

### üîå O Que Faz
- Executa modelos de linguagem diretamente no seu PC
- Roda o modelo `gemma:2b` para responder intera√ß√µes
- Disponibiliza agentes: **Ideator**, **Coder** e **Reviewer**
- Interface em modal para conversa em tempo real

### ‚öôÔ∏è Como Funciona
- Backend com Express escutando em `http://localhost:3001`
- Endpoint `/api/agent` envia mensagens ao Ollama local (`localhost:11434`)
- O modelo `gemma:2b` √© usado para gerar respostas
- Interface React com bot√£o e modal interativo

---

## üß† Stack T√©cnica

### üé® Front-End
- **React.js**, **Vite**
- **Styled Components**, **CSS puro**
- Componentes din√¢micos e responsivos

### ‚öôÔ∏è Back-End
- **Node.js**, **Express.js**
- Integra√ß√£o direta com Ollama via API local

### ü§ñ Ferramentas de IA
- **Ollama** como motor de infer√™ncia local
- Modelo **gemma:2b** com foco em NLP
- Respostas baseadas em prompt sem precisar da nuvem

---

## üóÇÔ∏è Estrutura do Projeto

```
/api
  ‚îî‚îÄ‚îÄ server.js         ‚Üê Servidor Express com integra√ß√£o Ollama
/src/components
  ‚îî‚îÄ‚îÄ AgentModal.jsx    ‚Üê UI do modal do agente com l√≥gica de chamada
```

---

## üí¨ Casos de Uso

- Assistente no portf√≥lio sem custo extra
- Provas de conceito com IA local
- Solu√ß√µes offline com interatividade real

---

## üì´ Contato

- üìß Email: [moacirsistemax@gmail.com](mailto:moacirsistemax@gmail.com)
- üßë‚Äçüíª GitHub: [@Moa-fernandes](https://github.com/Moa-fernandes)
- üíº LinkedIn: [Moacir Fernandes](https://www.linkedin.com/in/moacir-fernandes-ba0a97a0/)


## ‚ö° Vamos Construir Algo Incr√≠vel???

Estou sempre aberto a colabora√ß√µes interessantes ‚Äî seja para criar interfaces impactantes, resolver desafios t√©cnicos no back-end, integrar ferramentas de IA ou entregar produtos completos com performance e estilo. Bora conversar? üöÄ


## ####################################################################### ################################

## üáßüá∑ Instru√ß√µes de Uso

### üéØ Requisitos

* Node.js
* Docker
* Sanity CLI (`npm install -g sanity`)
* Ollama (opcional para uso local do agente IA)

---

### üìÅ 1. Portf√≥lio Front-End (`/`)

```bash
npm install
npm run dev
```

### ‚öôÔ∏è 2. API do Agente (`/api`)

```bash
cd api
npm install
node server.js
```

> Essa API se comunica com o Ollama local na porta `11434`. Certifique-se de ter o modelo `gemma:2b` carregado via `ollama pull gemma:2b` e rodando com `ollama run gemma:2b`.

---

### üß† 3. CMS Sanity (`/portfoliomoacms`)

```bash
cd portfoliomoacms
npm install
npm run dev  # Acessar via http://localhost:3333
npx sanity deploy  # Para publicar no sanity.studio
```

---

### üê≥ 4. Docker (Build & Run)

```bash
docker build -t portfolio-react .
docker run -p 80:80 portfolio-react
```

---

## üá∫üá∏ Usage Instructions

### üéØ Requirements

* Node.js
* Docker
* Sanity CLI (`npm install -g sanity`)
* Ollama (optional for offline local AI)

---

### üìÅ 1. Portfolio Front-End (`/`)

```bash
npm install
npm run dev
```

### ‚öôÔ∏è 2. Agent API (`/api`)

```bash
cd api
npm install
node server.js
```

> This API talks to local Ollama at port `11434`. Make sure you have the `gemma:2b` model pulled with `ollama pull gemma:2b` and running via `ollama run gemma:2b`.

---

### üß† 3. Sanity CMS (`/portfoliomoacms`)

```bash
cd portfoliomoacms
npm install
npm run dev  # Open on http://localhost:3333
npx sanity deploy  # Deploy to sanity.studio
```

---

### üê≥ 4. Docker (Build & Run)

```bash
docker build -t portfolio-react .
docker run -p 80:80 portfolio-react
```
